{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184ed0fb",
   "metadata": {},
   "source": [
    "### What is Hyperparameter Tuning?\n",
    "\n",
    "`Hyperparameter tuning` is the process of finding the best set of hyperparameters (configuration settings) for a machine learning algorithm.\n",
    "\n",
    "Unlike model parameters (learned from data), hyperparameters are set before training and control how the model learns.\n",
    "\n",
    "The goal is to improve model performance (accuracy, precision, recall, RMSE, etc.) by choosing optimal hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0999a7",
   "metadata": {},
   "source": [
    "### Parameters vs Hyperparameters  \n",
    "\n",
    "### Parameters  \n",
    "- Parameters are values **learned from the training data** by the model.  \n",
    "- They are updated during training using optimization algorithms (like gradient descent).  \n",
    "- They represent the \"knowledge\" the model has gained.  \n",
    "\n",
    "#### Examples:\n",
    "- **Linear Regression:** coefficients (weights) `β0, β1, β2, ...` and the intercept.  \n",
    "- **Logistic Regression:** weights associated with each feature and the bias.    \n",
    "- **KNN:** (no learned parameters, since it memorizes the dataset).  \n",
    "- **decision trees:** these are the actual splits and structure the model creates.\n",
    "- **SVM:** support vectors, weights, and bias learned from the data.\n",
    "- **Naive Bayes:** Prior Probabilities and the probability of each feature given a class.\n",
    "\n",
    "Example: In a Linear Regression equation:  \n",
    "\\[\n",
    "y = β_0 + β_1x_1 + β_2x_2 + ... + β_nx_n\n",
    "\\]  \n",
    "The `β`s are the **parameters** that the algorithm learns.  \n",
    "\n",
    "### Hyperparameters  \n",
    "- Hyperparameters are set **before training** and control how the model learns.  \n",
    "- They are **not learned** from the data but chosen by the data scientist (sometimes tuned using GridSearch, Random Search, etc.).  \n",
    "\n",
    "#### Examples:\n",
    "- **Logistic Regression:** `C`, `penalty`, `solver`.  \n",
    "- **Decision Trees:** `max_depth`, `min_samples_split`.  \n",
    "- **KNN:** `n_neighbors`, `metric`.  \n",
    "- **SVM:** `C`, `kernel`, `gamma`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a946b8",
   "metadata": {},
   "source": [
    "#### Common Hyperparameter Tuning Methods\n",
    "\n",
    "1. **Grid Search**\n",
    "- Tries all possible combinations of hyperparameters.\n",
    "- Computationally expensive.\n",
    "\n",
    "2. **Random Search**\n",
    "- Samples random combinations of hyperparameters.\n",
    "- Faster than grid search and often effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b3ca4",
   "metadata": {},
   "source": [
    "### Examples of Hyperparameters  \n",
    "\n",
    "#### 1. Linear Regression  \n",
    "- `fit_intercept` – whether to calculate the intercept (boolean).  \n",
    "- `normalize` – whether to normalize input features (deprecated in new versions).  \n",
    "\n",
    "#### 2. Logistic Regression  \n",
    "- `C` – inverse of regularization strength (smaller values = stronger regularization).  \n",
    "- `penalty` – type of regularization (`l1`, `l2`, `elasticnet`, `none`).  \n",
    "- `solver` – optimization algorithm (`liblinear`, `saga`, `newton-cg`, `lbfgs`).  \n",
    "- `max_iter` – maximum number of iterations for convergence.  \n",
    "\n",
    "#### 3. Decision Trees  \n",
    "- `max_depth` – maximum depth of the tree.  \n",
    "- `min_samples_split` – minimum number of samples required to split a node.  \n",
    "- `min_samples_leaf` – minimum number of samples required at a leaf node.  \n",
    "- `max_features` – number of features to consider for the best split.  \n",
    "- `criterion` – function to measure split quality (`gini`, `entropy`).  \n",
    "\n",
    "#### 4. Random Forest  \n",
    "Includes all Decision Tree hyperparameters, plus:  \n",
    "- `n_estimators` – number of trees in the forest.  \n",
    "- `bootstrap` – whether bootstrap samples are used when building trees.  \n",
    "\n",
    "#### 5. K-Nearest Neighbors (KNN)  \n",
    "- `n_neighbors` – number of neighbors to consider.  \n",
    "- `metric` – distance metric (`euclidean`, `manhattan`, `minkowski`).  \n",
    "- `weights` – how to weight neighbors (`uniform`, `distance`).  \n",
    "\n",
    "#### 6. Support Vector Machine (SVM)  \n",
    "- `C` – regularization parameter.  \n",
    "- `kernel` – type of kernel (`linear`, `rbf`, `poly`, `sigmoid`).  \n",
    "- `gamma` – kernel coefficient (affects influence of a single training example).  \n",
    "- `degree` – degree of polynomial kernel (if `poly`).  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
