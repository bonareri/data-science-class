{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5e0311",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models\n",
    "\n",
    "- The k-means clustering model explored in the previous section is simple and relatively easy to understand, but its simplicity leads to practical challenges in its application. \n",
    "- In particular, the non-probabilistic nature of k-means and its use of simple distance-from-cluster-center to assign cluster membership leads to poor performance for many real-world situations. \n",
    "- In this section we will take a look at `Gaussian mixture models (GMMs)`, which can be viewed as an extension of the ideas behind `k-means`, but can also be a powerful tool for estimation beyond simple clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b6534",
   "metadata": {},
   "source": [
    "### So What is a Gaussian Mixture Model (GMM)?\n",
    "\n",
    "A `Gaussian Mixture Model` is a probabilistic model that assumes your data is generated from a mixture of several Gaussian (normal) distributions with unknown parameters, each corresponding to a cluster. \n",
    "\n",
    "Unlike simple clustering methods that assign each data point to a single cluster, GMM incorporates the concept of probability and uncertainty. \n",
    "\n",
    "`Example`: A person 150 cm tall might be 80% likely to be a child, 20% likely to be an adult → GMM captures this uncertainty. \n",
    "\n",
    "This allows for more flexible cluster shapes as well as soft clustering, where data points can belong to multiple clusters with varying degrees of membership."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b84ce8",
   "metadata": {},
   "source": [
    "### Key Concepts and Terminologies in GMM\n",
    "\n",
    "#### 1. Gaussian Distribution: \n",
    "\n",
    "Also known as the `normal distribution`, characterized by its bell-shaped curve, defined primarily by its mean (center) and variance (width).\n",
    "\n",
    "#### 2. Mixture Model\n",
    "\n",
    "- Instead of assuming data comes from one distribution, we assume it comes from a combination (mixture) of multiple Gaussians.\n",
    "- Each Gaussian corresponds to a cluster.\n",
    "\n",
    "#### 3. Components\n",
    "\n",
    "- Each Gaussian in the mixture is called a `component`.\n",
    "- If you set k=3, your model assumes there are 3 Gaussian components (clusters).\n",
    "\n",
    "#### 4. Soft Assignment\n",
    "\n",
    "- Unlike K-Means (hard assignment: one point = one cluster),\n",
    "- GMM assigns probabilities of belonging to each cluster.\n",
    "- Example: A point could be 70% cluster A, 30% cluster B.\n",
    "\n",
    "### 5. Covariance Types\n",
    "\n",
    "Covariance controls the shape of each Gaussian cluster.\n",
    "\n",
    "- Spherical → same variance in all directions (circular blobs).\n",
    "- Diagonal → axis-aligned ellipses.\n",
    "- Full → ellipses of any orientation.\n",
    "\n",
    "#### Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "- The optimization method used to fit GMMs:\n",
    "- `E-Step (Expectation)`: Calculate probabilities (responsibilities) that each point belongs to each cluster.\n",
    "- `M-Step (Maximization)`: Update means, covariances, and weights using those probabilities.\n",
    "- Repeat until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131bd3aa",
   "metadata": {},
   "source": [
    "### How it works\n",
    "\n",
    "1. `Assumption`: Your data points come from k different Gaussian distributions (clusters).\n",
    "2. Each cluster has:\n",
    "    - A `mean` (center of the Gaussian).\n",
    "    - A `covariance matrix` (shape/spread of the Gaussian).\n",
    "    - A `mixing weight` (how much of the data belongs to that cluster).\n",
    "3. GMM tries to learn these parameters using the Expectation-Maximization (EM) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0259f52",
   "metadata": {},
   "source": [
    "### Advantages of GMM Over Traditional Clustering Techniques\n",
    "- `Flexibility in Cluster Covariance`: GMM allows for clusters to have different shapes and sizes, adapting to the intrinsic distribution of the data rather than assuming all clusters are spherical (as in K-means).\n",
    "- `Soft Clustering Capabilities`: Unlike hard clustering methods that assign each data point to a single cluster, GMM assigns a probability to each data point for belonging to each of the mixture components, allowing for a more nuanced understanding of data groupings.\n",
    "- `Modeling Complex Distributions`: GMM can model complex distributions that may be multimodal (having multiple peaks), which is a significant advantage in real-world data analysis where single-mode assumptions (one peak per cluster) are often insufficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
