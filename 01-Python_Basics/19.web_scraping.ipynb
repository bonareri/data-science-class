{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "420c1440",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "Web scraping, the process of extracting data from websites, has emerged as a powerful technique to gather information from the vast expanse of the internet. \n",
    "\n",
    "**Beautiful Soup** is a popular Python library that makes it easy to scrape information from web pages.\n",
    "\n",
    "---\n",
    "\n",
    "### Importance of Web Scraping\n",
    "\n",
    "#### 1. Data Collection and Aggregation\n",
    "- **Market Research**: Gather insights about competitors, market trends, and customer preferences.\n",
    "- **Price Monitoring**: E-commerce platforms can dynamically adjust prices based on competitor data.\n",
    "- **News Aggregation**: Pull articles from various sources for centralized, real-time news coverage.\n",
    "\n",
    "#### 2. Business Intelligence and Analytics\n",
    "- **Customer Sentiment Analysis**: Extract reviews and social media comments to improve products/services.\n",
    "- **Trend Analysis**: Identify industry patterns using scraped data from multiple sources.\n",
    "\n",
    "#### 3. Content Extraction\n",
    "- **Academic Research**: Automate data collection for analysis in research papers.\n",
    "- **Data Journalism**: Support investigative journalism with structured and verifiable data.\n",
    "\n",
    "#### 4. Lead Generation\n",
    "- **Contact Information**: Extract emails and phone numbers for marketing campaigns.\n",
    "- **Job Listings**: Aggregate listings across sites to assist job seekers and recruitment platforms.\n",
    "\n",
    "#### 5. SEO and SEM Strategies\n",
    "- **Keyword Research**: Analyze keywords used by competitors for SEO optimization.\n",
    "- **Backlink Analysis**: Discover competitor backlink sources to improve your domain authority.\n",
    "\n",
    "#### 6. Automating Repetitive Tasks\n",
    "- **Data Entry**: Reduce manual effort and error by automating data capture.\n",
    "- **Monitoring**: Track changes and updates to websites automatically.\n",
    "\n",
    "#### 7. Personal Projects and Learning\n",
    "- **Portfolio Projects**: Showcase your skills in data collection and processing.\n",
    "- **Learning and Experimentation**: Practice and explore Python, HTML parsing, and data analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### Ethical and Legal Considerations\n",
    "\n",
    "While web scraping is powerful, it's essential to use it **responsibly and ethically**:\n",
    "\n",
    "- **Respect Terms of Service**: Always check if the website permits scraping.\n",
    "- **Respect `robots.txt`**: Follow the site’s crawling policies.\n",
    "- **Data Privacy**: Avoid scraping personal data unless consent is given and comply with laws like **GDPR**.\n",
    "- **Server Load**: Don’t overload servers with rapid, repeated scraping — it can lead to denial-of-service issues.\n",
    "\n",
    "---\n",
    "\n",
    "> ⚠️ Always scrape **politely** and **ethically**. Use user-agents, time delays, and handle retries/errors gracefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a55bc",
   "metadata": {},
   "source": [
    "## Introduction to Beautiful Soup\n",
    "\n",
    "**Beautiful Soup** is a Python library used to parse HTML and XML documents. It provides simple methods to **search**, **navigate**, and **modify** the parse tree. Beautiful Soup is designed to be easy and beginner-friendly, making it an excellent tool for web scraping projects.\n",
    "\n",
    "With Beautiful Soup, you can extract data from websites to:\n",
    "- Create reports\n",
    "- Visualize data\n",
    "- Perform analysis\n",
    "\n",
    "---\n",
    "\n",
    "### What is Data Parsing?\n",
    "\n",
    "**Data parsing** refers to the process of converting raw data (such as HTML) into a different format that's easier to work with.\n",
    "\n",
    "#### What Does a Data Parser Do?\n",
    "\n",
    "A **data parser**:\n",
    "1. Receives data in a certain format (e.g., HTML).\n",
    "2. Reads the data and stores it as a string.\n",
    "3. Extracts relevant information from the string.\n",
    "4. Optionally cleans or processes the data.\n",
    "5. Outputs it in formats like JSON, CSV, YAML, or stores it in databases.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Why Parsing is Useful\n",
    "\n",
    "**Imagine you're building a price comparison tool**:\n",
    "- It scrapes data from multiple e-commerce sites.\n",
    "- Collects and compares product prices in real time.\n",
    "- Helps users find the best deals.\n",
    "- Increases your traffic and affiliate sales.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Guide to Web Scraping Using Beautiful Soup\n",
    "\n",
    "#### Step 1: Install Required Libraries\n",
    "\n",
    "Install using `pip`:\n",
    "```bash\n",
    "pip install bs4\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "#### Step 2: Import Libraries\n",
    "```python \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "\n",
    "#### Step 3: Send an HTTP Request\n",
    "```python \n",
    "url = 'http://example.com'\n",
    "response = requests.get(url)\n",
    "```\n",
    "\n",
    "#### Step 4: Parse the HTML Content\n",
    "```python\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "```\n",
    "\n",
    "#### Step 5: Extract Data\n",
    "- Example: Extract all ```<h1>``` tags\n",
    "```python\n",
    "h1_tags = soup.find_all('h1')\n",
    "for h1 in h1_tags:\n",
    "    print(h1.text)\n",
    "```\n",
    "\n",
    "#### Step 6: More Advanced Data Extraction\n",
    "- Find by Tag Name\n",
    "\n",
    "```python\n",
    "title = soup.title\n",
    "print(title.text)\n",
    "```\n",
    "\n",
    "- Find by Class Name\n",
    "\n",
    "```python\n",
    "articles = soup.find_all('div', class_='article')\n",
    "for article in articles:\n",
    "    print(article.text)\n",
    "```\n",
    "\n",
    "- Find by ID \n",
    "```python\n",
    "main_content = soup.find(id='main-content')\n",
    "print(main_content.text)\n",
    "```\n",
    "\n",
    "- Extract Attributes\n",
    "```python\n",
    "img_tags = soup.find_all('img')\n",
    "for img in img_tags:\n",
    "    print(img['src'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb6cd2",
   "metadata": {},
   "source": [
    "### Example Project: Scraping an Online Bookstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d72745",
   "metadata": {},
   "source": [
    "#### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06b18765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   #Fetch content from the web.\n",
    "from bs4 import BeautifulSoup # Parse HTML content.\n",
    "import csv # Save data to a CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c6b11",
   "metadata": {},
   "source": [
    "#### Step 2: Send HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "783874f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(\"http://books.toscrape.com/\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837050d4",
   "metadata": {},
   "source": [
    "#### Step 3: Parse HTML Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00a15b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28098e",
   "metadata": {},
   "source": [
    "| Part                 | Meaning                                                                                                                                                                        |\n",
    "| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `response.content`   | This is the **raw HTML content** (in bytes) returned from the website by the `requests.get()` call. It's what the browser would see when loading the page.                     |\n",
    "| `'html.parser'`      | This is the **parser** used by Beautiful Soup to process the HTML. It tells Beautiful Soup how to interpret the structure of the HTML document.                                |\n",
    "| `BeautifulSoup(...)` | This creates a **BeautifulSoup object**, which represents the entire HTML document. This object provides powerful methods to extract elements like `<div>`, `<p>`, `<a>`, etc. |\n",
    "| `soup = ...`         | Stores the parsed result in a variable named `soup`. This is your main entry point to navigate and extract information from the webpage.                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6256ad",
   "metadata": {},
   "source": [
    "####  Step 4: Find Book Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d5cf16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find product containers using find_all()\n",
    "books = soup.find_all('article', class_='product_pod')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1033da",
   "metadata": {},
   "source": [
    "#### Step 5: Extract Book Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc1b81a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A Light in the Attic', '£51.77', 'In stock'],\n",
       " ['Tipping the Velvet', '£53.74', 'In stock'],\n",
       " ['Soumission', '£50.10', 'In stock'],\n",
       " ['Sharp Objects', '£47.82', 'In stock'],\n",
       " ['Sapiens: A Brief History of Humankind', '£54.23', 'In stock'],\n",
       " ['The Requiem Red', '£22.65', 'In stock'],\n",
       " ['The Dirty Little Secrets of Getting Your Dream Job', '£33.34', 'In stock'],\n",
       " ['The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull',\n",
       "  '£17.93',\n",
       "  'In stock'],\n",
       " ['The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics',\n",
       "  '£22.60',\n",
       "  'In stock'],\n",
       " ['The Black Maria', '£52.15', 'In stock'],\n",
       " ['Starving Hearts (Triangular Trade Trilogy, #1)', '£13.99', 'In stock'],\n",
       " [\"Shakespeare's Sonnets\", '£20.66', 'In stock'],\n",
       " ['Set Me Free', '£17.46', 'In stock'],\n",
       " [\"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\",\n",
       "  '£52.29',\n",
       "  'In stock'],\n",
       " ['Rip it Up and Start Again', '£35.02', 'In stock'],\n",
       " ['Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991',\n",
       "  '£57.25',\n",
       "  'In stock'],\n",
       " ['Olio', '£23.88', 'In stock'],\n",
       " ['Mesaerion: The Best Science Fiction Stories 1800-1849',\n",
       "  '£37.59',\n",
       "  'In stock'],\n",
       " ['Libertarianism for Beginners', '£51.33', 'In stock'],\n",
       " [\"It's Only the Himalayas\", '£45.17', 'In stock']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for book in books:\n",
    "    title = book.h3.a['title']\n",
    "    price = book.find('p', class_='price_color').text\n",
    "    availability = book.find('p', class_=\"instock availability\").text.strip()\n",
    "    data.append([title, price, availability])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbaa1b",
   "metadata": {},
   "source": [
    "#### Step 6: Write Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36114fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to bookstore.csv\n"
     ]
    }
   ],
   "source": [
    "with open('bookstore.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Price\", \"Availability\"])\n",
    "    writer.writerows(data)\n",
    "print(\"Data has been written to bookstore.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae527616",
   "metadata": {},
   "source": [
    "#### Advanced Extraction Techniques\n",
    "\n",
    "- ```soup.find_all('div', class_='some_class')```\n",
    "- Extracting links, images, and nested elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5eaf593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\n",
      "media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f4a1c.jpg\n",
      "media/cache/3e/ef/3eef99c9d9adef34639f510662022830.jpg\n",
      "media/cache/32/51/3251cf3a3412f53f339e42cac2134093.jpg\n",
      "media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c12a6.jpg\n",
      "media/cache/68/33/68339b4c9bc034267e1da611ab3b34f8.jpg\n",
      "media/cache/92/27/92274a95b7c251fea59a2b8a78275ab4.jpg\n",
      "media/cache/3d/54/3d54940e57e662c4dd1f3ff00c78cc64.jpg\n",
      "media/cache/66/88/66883b91f6804b2323c8369331cb7dd1.jpg\n",
      "media/cache/58/46/5846057e28022268153beff6d352b06c.jpg\n",
      "media/cache/be/f4/bef44da28c98f905a3ebec0b87be8530.jpg\n",
      "media/cache/10/48/1048f63d3b5061cd2f424d20b3f9b666.jpg\n",
      "media/cache/5b/88/5b88c52633f53cacf162c15f4f823153.jpg\n",
      "media/cache/94/b1/94b1b8b244bce9677c2f29ccc890d4d2.jpg\n",
      "media/cache/81/c4/81c4a973364e17d01f217e1188253d5e.jpg\n",
      "media/cache/54/60/54607fe8945897cdcced0044103b10b6.jpg\n",
      "media/cache/55/33/553310a7162dfbc2c6d19a84da0df9e1.jpg\n",
      "media/cache/09/a3/09a3aef48557576e1a85ba7efea8ecb7.jpg\n",
      "media/cache/0b/bc/0bbcd0a6f4bcd81ccb1049a52736406e.jpg\n",
      "media/cache/27/a5/27a53d0bb95bdd88288eaf66c9230d7e.jpg\n"
     ]
    }
   ],
   "source": [
    "images = soup.find_all('img')\n",
    "for img in images:\n",
    "    print(img['src'])  # Print the source URL of each image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535845f5",
   "metadata": {},
   "source": [
    "### Scrape Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2182d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an HTTP Request\n",
    "url = \"http://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b047bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quotes to Scrape\n"
     ]
    }
   ],
   "source": [
    "title = soup.title.text\n",
    "print(title)  # Print the title of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "295840e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find All Quotes\n",
    "# Each quote is inside a <div class=\"quote\"> block.\n",
    "quotes = soup.find_all('div', class_ = 'quote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0be26f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting quotes, authors, and tags\n",
    "quotes_data = []\n",
    "for quote in quotes:\n",
    "    text = quote.find('span', class_='text').text\n",
    "    author = quote.find('small', class_='author').text\n",
    "    tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
    "    quotes_data.append({\n",
    "        'text': text,\n",
    "        'author': author,\n",
    "        'tags': tags\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f9d5da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quotes have been written to quotes.csv\n"
     ]
    }
   ],
   "source": [
    "# save the data to a CSV file\n",
    "with open('quotes.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Text', 'Author', 'Tags'])\n",
    "\n",
    "    for quote in quotes_data:\n",
    "        writer.writerow([quote['text'], quote['author'], ', '.join(quote['tags'])])\n",
    "print(\"Quotes have been written to quotes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ab152",
   "metadata": {},
   "source": [
    "```python\n",
    "<li class=\"next\">\n",
    "  <a href=\"/page/2/\">Next →</a>\n",
    "</li>\n",
    "```\n",
    "- The href=\"/page/2/\" is a relative URL, not the full link.\n",
    "- It only gives the part after the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d4837",
   "metadata": {},
   "source": [
    "#### Challenge:  Scraping multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff2ddb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: http://quotes.toscrape.com/\n",
      "Scraping page: http://quotes.toscrape.com/page/2/\n",
      "Scraping page: http://quotes.toscrape.com/page/3/\n",
      "Scraping page: http://quotes.toscrape.com/page/4/\n",
      "Scraping page: http://quotes.toscrape.com/page/5/\n",
      "Scraping page: http://quotes.toscrape.com/page/6/\n",
      "Scraping page: http://quotes.toscrape.com/page/7/\n",
      "Scraping page: http://quotes.toscrape.com/page/8/\n",
      "Scraping page: http://quotes.toscrape.com/page/9/\n",
      "Scraping page: http://quotes.toscrape.com/page/10/\n",
      "All quotes scraped and saved to quotes_all_pages.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "base_url = \"http://quotes.toscrape.com\"  # constant part of the site\n",
    "url = \"/\"  # changing part like /, /page/2/\n",
    "\n",
    "quotes_data = []\n",
    "\n",
    "while url:\n",
    "    full_url = base_url + url\n",
    "    print(f\"Scraping page: {full_url}\")  # Message for each page\n",
    "\n",
    "    # Send HTTP request to current page\n",
    "    response = requests.get(full_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract quotes, authors, tags\n",
    "    quotes = soup.find_all('div', class_='quote')\n",
    "    for quote in quotes:\n",
    "        text = quote.find('span', class_='text').text\n",
    "        author = quote.find('small', class_='author').text\n",
    "        tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
    "        quotes_data.append({\n",
    "            'text': text,\n",
    "            'author': author,\n",
    "            'tags': tags\n",
    "        })\n",
    "\n",
    "    # Check for the next page\n",
    "    next_btn = soup.find('li', class_='next')\n",
    "    if next_btn:\n",
    "        url = next_btn.a['href']\n",
    "    else:\n",
    "        url = None\n",
    "\n",
    "# Write to CSV\n",
    "with open('quotes_all_pages.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Text', 'Author', 'Tags'])\n",
    "    for quote in quotes_data:\n",
    "        writer.writerow([quote['text'], quote['author'], ', '.join(quote['tags'])])\n",
    "\n",
    "print(\"All quotes scraped and saved to quotes_all_pages.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
